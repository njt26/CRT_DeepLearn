{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In today's lab, we'll build a simple feedforward neural network, and train it to compute the XOR function. We'll do this in core Python (plus NumPy) first: it's important to see every detail at least once. Then we'll repeat the exercise in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick demo of linear algebra operations in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([1,-2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -2,  3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 12, 18, 24])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix acting on a vector\n",
    "matrix @ vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product of two vectors\n",
    "vector @ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR 'truth table' as NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our neural net to learn to compute this function. (Notice that linear regression can't solve this problem -- at least, not unless we include an interaction term. The XOR function is non-linear.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_table = np.array([[0,0,0], [0,1,1], [1,0,1], [1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs in cols 1 and 2, output in col 3\n",
    "xor_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for training a neural network to compute XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train a neural network to compute XOR by showing it lots of slightly noisy examples of correctly evaluated XORs. We'll add noise to the inputs (the 'feature vectors' in machine learning parlance), but leave the desired outputs (the 'labels') untouched. Our neural net should eventually learn that the output should be roughly 1 if the feature vector is close to (0,1) or (1,0), and roughly zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate slightly noisy feature vectors, and noise-free labels.\n",
    "'''\n",
    "n = 100 # total number of cases\n",
    "noise_level = 0.2 # amplitude of Gaussian noise\n",
    "data = xor_table[np.random.choice(range(4), size = n), :] # sample with replacement from XOR table\n",
    "noise = np.concatenate((noise_level * np.random.randn(n,2), np.zeros((n,1))), axis=1) # noise to inject into data\n",
    "data = data + noise\n",
    "features = data[:,:2]\n",
    "labels = data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect what we've assembled so far by looking at the first few rows of a few of these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83838459,  1.00932889,  0.        ],\n",
       "       [-0.15055696,  0.24436154,  0.        ],\n",
       "       [-0.12628471,  0.03940612,  0.        ],\n",
       "       [ 0.28210009,  0.1937349 ,  0.        ],\n",
       "       [ 0.80609337,  1.06311941,  0.        ],\n",
       "       [ 0.08992244,  0.74923709,  1.        ],\n",
       "       [ 0.89738992,  0.81574618,  0.        ],\n",
       "       [ 0.90330091,  0.89253292,  0.        ],\n",
       "       [ 1.23942807,  0.90476843,  0.        ],\n",
       "       [ 0.90138015,  0.15160295,  1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.83838459,  1.00932889],\n",
       "       [-0.15055696,  0.24436154],\n",
       "       [-0.12628471,  0.03940612],\n",
       "       [ 0.28210009,  0.1937349 ],\n",
       "       [ 0.80609337,  1.06311941],\n",
       "       [ 0.08992244,  0.74923709],\n",
       "       [ 0.89738992,  0.81574618],\n",
       "       [ 0.90330091,  0.89253292],\n",
       "       [ 1.23942807,  0.90476843],\n",
       "       [ 0.90138015,  0.15160295]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Implement a neural network with two inputs, a single hidden layer of two ReLU neurons, and a single linear output neuron. Do this by writing a `relu(x)` function, and a  `forward_pass(feature_vector, weights)` function. The latter should take a feature vector (e.g. (1,0)) and a matrix of weights, and compute the output of the network with those weights when fed that input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(feature_vector, weights):\n",
    "    '''\n",
    "    Implements feed-forward NN with two inputs, two hidden-layer units (ReLUs),\n",
    "    and a single output unit (linear).\n",
    "    feature_vector is the input: (x1, x2).\n",
    "    First two rows of weights are 1st-layer biases and weights: [b1, a11, a12], [b2, a21, a22].\n",
    "    (NB that a_ij is weight for connection *to* ith hidden layer neuron *from* jth input.)\n",
    "    Final row of weights are 2nd-layer biases and weights: [b, w1, w2].\n",
    "    '''\n",
    "    feature_vector = np.append(1, feature_vector) # add dummy input to play nicely with biases\n",
    "    hidden_layer_output = relu(weights[:2] @ feature_vector)\n",
    "    hidden_layer_output = np.append(1, hidden_layer_output) # add dummy again\n",
    "    return (weights[2:] @ hidden_layer_output)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding good weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining design task is to pick a set of weights that will do the desired job (i.e. compute  XOR). We *could* do this manually -- through some combination of cunning and trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Have a go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one possibility. (Draw a diagram and think through how it works.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_guess = np.array([[0,1,-1], [0,-1,1], [0,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] 0\n",
      "[0 1] 1\n",
      "[1 0] 1\n",
      "[1 1] 0\n"
     ]
    }
   ],
   "source": [
    "for row in xor_table[:,:-1]:\n",
    "    print(row, forward_pass(row, weights_guess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking weights 'manually' won't work for bigger, more complicated problems; and in any case the whole point of deep learning is for networks to learn the right weights algorithmically -- from the data. This process is known as **training**. The standard approach is to initialise weights to random values and then hunt for better choices by doing gradient descent (or something similar). For that we need a loss function. We'll use a simple one for now: mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(feature_vectors, labels, weights):\n",
    "    predictions = [forward_pass(vect, weights) for vect in feature_vectors]\n",
    "    errors = predictions - labels\n",
    "    return (errors**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, randomly assigned weights do not do a good job computing XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.uniform(-1.2, 1.2, size = (3,3)) # initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.79646998, -0.13308253,  0.09449428],\n",
       "       [ 0.65721397, -0.0535835 , -0.91239527],\n",
       "       [ 1.11872704,  0.9829142 , -0.86812605]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.451417614125789"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(features, labels, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply gradient descent on the loss function, we need to be able to calculate the gradient of the loss function with respect to each component of the weights matrix in turn. The natural object to store this gradient in is a matrix with the same shape as the weights matrix.\n",
    "\n",
    "The function below computes each gradient component by testing the effect on the loss of a small change in the corresponding weight. (In a deep network, it would be very inefficient to run through this procedure separately for each weight component -- but we won't worry about that now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_gradient(feature_vectors, labels, weights, delta = 0.01):\n",
    "    current_loss = loss(feature_vectors, labels, weights)\n",
    "    gradient = np.empty(weights.shape) # initialise empty matrix to hold gradient information\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            weights[i,j] += delta # perturb weight\n",
    "            perturbed_loss = loss(feature_vectors, labels, weights)\n",
    "            weights[i,j] -= delta # restore weight\n",
    "            gradient[i,j] = (perturbed_loss - current_loss) / delta\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [-0.16875602,  0.17938838, -0.03970261],\n",
       "       [ 0.74800735,  0.        ,  0.09893411]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_gradient(features, labels, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.** If we are going to change the weights by taking a small step in 'weights space', what direction should we step in?\n",
    "\n",
    "**A.** The direction in which the loss is *decreasing* most quickly. That's in the direction `-loss_gradient`.\n",
    "\n",
    "Let's take just one step and confirm that the loss has decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1 # controls sizes of steps\n",
    "weights = weights - learning_rate * loss_gradient(features, labels, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.79646998, -0.13308253,  0.09449428],\n",
       "       [ 0.67408957, -0.07152234, -0.90842501],\n",
       "       [ 1.0439263 ,  0.9829142 , -0.87801946]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3960301957218067"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(features, labels, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the loss has indeed decreased. That's not surprising, but it wasn't a sure thing: if we'd picked too large a value for `learning_rate`, the loss might have increased. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can *iterate* the above procedure. The hope is that by doing so, we'll converge on a really good set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Try this. Write code to perform 100 further rounds of gradient descent. (And use a smaller learning rate, just to be on the safe side.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss:  0.3960301957218067\n",
      "Final loss:  0.23310575448856008\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "print('Initial loss: ', loss(features, labels, weights))\n",
    "for i in range(100):\n",
    "    weights = weights - learning_rate * loss_gradient(features, labels, weights)\n",
    "print('Final loss: ', loss(features, labels, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the loss, we can compute the accuracy: the fraction of cases the network is getting right. We'll say it gets a case right if the output rounded to the nearest integer matches the ground-truth label (i.e., the desired XOR value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.66\n"
     ]
    }
   ],
   "source": [
    "best_guesses = np.array([round(forward_pass(vect, weights)) for vect in features])\n",
    "print('Accuracy: ', sum(best_guesses == labels) / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion question**. Why would accuracy be a poor choice of loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the network's output for all four possible 'pristine' (non-noise-corrupted) cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] 0.21674630030615893\n",
      "[0 1] 0.7439746954638509\n",
      "[1 0] 0.48623895139901346\n",
      "[1 1] 0.7439746954638509\n"
     ]
    }
   ],
   "source": [
    "for row in xor_table[:,:-1]:\n",
    "    print(row, forward_pass(row, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you re-run the above training process many times (starting from the top, with random initial weights), you will *sometimes* converge to a good solution; but rather often you will converge to a poor solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Demonstrate this. (Reuse the code you already have.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So getting lucky with the initial weights seems to be important. We could keep picking new sets of initial weights and running gradient descent until we end up somewhere good. But could we instead *revise the architecture* of the network to make it more likely that a random initial set of weights will lead us (via gradient descent) to a good solution?\n",
    "\n",
    "Yes. The trick is to use a bigger network, with more neurons than are strictly necessary for computing XOR. You can think of this as being a bit like a parallel version of the 'pick random weights, train, and start again if you get stuck' strategy. With a larger network, the neurons may not all end up (post-training) doing a useful job; but there's a better chance that *some subset* of the initial weights is such that gradient descent will take us to a good solution.\n",
    "\n",
    "We'll eventually demonstrate this point, but not with our home-brew neural network code. It's time to introduce TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First glimpse of TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow lets you design, train, and run neural networks using a high-level API. Fast C code runs under the hood. Here's a reimplementation of our simple XOR network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(2,), activation='relu')) \n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', \n",
    "              metrics=['binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3719959378242493, 0.57)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(features, labels, epochs=100, verbose=0)\n",
    "history.history['loss'][-1], history.history['binary_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's an implementation of a lager network, with two hidden layers (of 8 and 4 neurons respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 8)                 24        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(8, input_shape=(2,), activation='relu')) \n",
    "model2.add(Dense(4, activation='relu'))\n",
    "model2.add(Dense(1))\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', \n",
    "              metrics=['binary_accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Why does the above network have 65 trainable parameters? Make sure you can account for all of them (and that you understand why there are not more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12737502952416738, 0.8666667)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model2.fit(features, labels, epochs=50, verbose=0, validation_split=0.25)\n",
    "history.history['loss'][-1], history.history['binary_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**. Re-run the above two training processes several times each. You should find that the larger network is less likely to get stuck at a bad local optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XOR problem is a pretty trivial, of course. But it's allowed us to build some intuition for why a bigger network (or what a statistician would call an *over-parameterised model*) might be easier to train. In this course, we will encounter networks with tens of thousands of trainable parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
